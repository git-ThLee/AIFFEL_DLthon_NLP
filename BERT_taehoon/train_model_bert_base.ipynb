{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "d987d6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 30 17:48:22 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   69C    P0    31W /  70W |   8654MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# GPU 확인 \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "0bf5ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "d1af3186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19053"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# 메모리 해제\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "afcb4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from soynlp.normalizer import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "db7a473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "    \n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "    \n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        # input_id는 워드 임베딩을 위한 문장의 정수 인코딩\n",
    "        input_id = tokenizer.encode(example, \n",
    "                                    max_length=max_seq_len, \n",
    "                                    pad_to_max_length=True,\n",
    "                                   )\n",
    "        \n",
    "        # attention_mask는 실제 단어가 위치하면 1, 패딩의 위치에는 0인 시퀀스\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "        \n",
    "        # token_type_id은 세그먼트 인코딩\n",
    "        token_type_id = [0] * max_seq_len\n",
    "        \n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "    \n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    \n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "    \n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "8fab2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForMultiClassClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.3):\n",
    "        super(TFBertForMultiClassClassification, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                activation='softmax', \n",
    "                                                name='classifier')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, attention_mask, token_type_ids = inputs\n",
    "        outputs = self.bert(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        cls_token = outputs[1]\n",
    "        if training:\n",
    "            cls_token = self.dropout(cls_token, training=training)\n",
    "        prediction = self.classifier(cls_token)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "63fbe91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFRobertaModel, RobertaTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"klue/roberta-large\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "\n",
    "class TFRobertaForMultiClassClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.3):\n",
    "        super(TFRobertaForMultiClassClassification, self).__init__()\n",
    "        self.roberta = AutoTokenizer.from_pretrained(model_name, from_pt=True)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                activation='softmax', \n",
    "                                                name='classifier')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, attention_mask = inputs  # RoBERTa는 token_type_ids가 필요하지 않습니다.\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = outputs[1]\n",
    "        if training:\n",
    "            cls_token = self.dropout(cls_token, training=training)\n",
    "        prediction = self.classifier(cls_token)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "cda3095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    emoticon_normalize(sentence)  # 이모티콘을 정규화합니다.\n",
    "    repeat_normalize(sentence)    # 반복되는 문자를 정규화합니다.\n",
    "    sentence = re.sub(r'([^a-zA-Zㄱ-ㅎ가-힣])', \" \", sentence)  # 영문, 한글 및 자음/모음을 제외한 문자를 공백으로 치환합니다.\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  # 연속된 공백을 하나의 공백으로 치환합니다.\n",
    "    sentence = sentence.strip()  # 문장의 양 끝에 있는 공백을 제거합니다.\n",
    "    return sentence  # 전처리된 문장을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4d41de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    # 중복 제거 \n",
    "    df = df.drop_duplicates(subset=['conversation']) \n",
    "    \n",
    "    # 문장 정규화\n",
    "    df['conversation'] = df['conversation'].apply(preprocess_sentence)\n",
    "    \n",
    "    # 결측치 제거\n",
    "    df = df.dropna(subset=['conversation'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "0e08552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_text_to_num(df):\n",
    "    # 레이블 값을 숫자로 매핑\n",
    "    label_mapping = {\n",
    "        '협박 대화': 0,\n",
    "        '갈취 대화': 1,\n",
    "        '직장 내 괴롭힘 대화': 2,\n",
    "        '기타 괴롭힘 대화': 3\n",
    "    }\n",
    "\n",
    "    df['class'] = df['class'].map(label_mapping)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "e6b5b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(x,y,classs):\n",
    "    def random_deletion(words, p=0.1):\n",
    "        if len(words) == 1:\n",
    "            return words\n",
    "\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            r = random.uniform(0, 1)\n",
    "            if r > p:\n",
    "                new_words.append(word)\n",
    "\n",
    "        if len(new_words) == 0:\n",
    "            rand_int = random.randint(0, len(words)-1)\n",
    "            return [words[rand_int]]\n",
    "\n",
    "        return ''.join(new_words)\n",
    "\n",
    "    def swap_word(new_words):\n",
    "        n = 5\n",
    "        for _ in range(n):\n",
    "            random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "            random_idx_2 = random_idx_1\n",
    "            counter = 0\n",
    "\n",
    "            while random_idx_2 == random_idx_1:\n",
    "                random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "                counter += 1\n",
    "                if counter > 3:\n",
    "                    return new_words\n",
    "\n",
    "            new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "    def random_swap(words):\n",
    "        new_words = list()\n",
    "        for word in words:\n",
    "            new_words.append(swap_word(word.split()))\n",
    "\n",
    "        return new_words\n",
    "    df = pd.concat([x,y],axis=1).reset_index(drop=True)\n",
    "    df_rd = df[df['class']==classs].copy()\n",
    "    df_rd['conversation'] = df_rd['conversation'].apply(random_deletion)\n",
    "    df_rs = df[df['class']==classs].copy()\n",
    "    df_rs['conversation'] = random_swap(df_rs['conversation'].values)\n",
    "    \n",
    "    df_concated = pd.concat([df, df_rs])\n",
    "#     df_concated = df_concated.reset_index(drop=True)\n",
    "    return df_concated.loc[:,['conversation']] , df_concated['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc6ae0",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "dfee21ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_557/1256559831.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['conversation'] = df['conversation'].apply(preprocess_sentence)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'bert.embeddings.position_ids', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "  0%|          | 0/3081 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 3081/3081 [00:01<00:00, 1668.32it/s]\n",
      "100%|██████████| 771/771 [00:00<00:00, 1729.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1541/1541 [==============================] - 228s 138ms/step - loss: 0.7906 - accuracy: 0.7189 - val_loss: 1.1080 - val_accuracy: 0.7095\n",
      "Epoch 2/10\n",
      "1541/1541 [==============================] - 210s 136ms/step - loss: 0.4779 - accuracy: 0.8569 - val_loss: 0.5353 - val_accuracy: 0.8521\n",
      "Epoch 3/10\n",
      "1541/1541 [==============================] - 209s 136ms/step - loss: 0.2181 - accuracy: 0.9400 - val_loss: 0.4728 - val_accuracy: 0.8599\n",
      "Epoch 4/10\n",
      "1541/1541 [==============================] - 209s 135ms/step - loss: 0.1629 - accuracy: 0.9542 - val_loss: 0.4680 - val_accuracy: 0.8638\n",
      "Epoch 5/10\n",
      "1541/1541 [==============================] - 208s 135ms/step - loss: 0.0960 - accuracy: 0.9760 - val_loss: 0.4597 - val_accuracy: 0.8638\n",
      "Epoch 6/10\n",
      "  74/1541 [>.............................] - ETA: 3:03 - loss: 0.0449 - accuracy: 0.9865"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_557/3104808660.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# data load \n",
    "train = pd.read_csv(train_path,index_col=0)\n",
    "\n",
    "# preprocessing\n",
    "train = preprocessing(train)\n",
    "\n",
    "\n",
    "# 라벨 숫자 변환\n",
    "train = class_text_to_num(train)\n",
    "\n",
    "# 기타 에서 돈 단어 제거\n",
    "train['conversation'] = train[['class','conversation']].apply(lambda x : x[1].replace('돈','') if x[0] == 3 else x[1], axis=1)\n",
    "# 협박 에서 돈 단어 제거 \n",
    "train['conversation'] = train[['class','conversation']].apply(lambda x : x[1].replace('돈','') if x[0] == 0 else x[1], axis=1)\n",
    "\n",
    "\n",
    "# 모델 load \n",
    "if model_name == 'klue/bert-base':\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = TFBertForMultiClassClassification(model_name, class_num)\n",
    "elif model_name == 'klue/roberta-small':\n",
    "    tokenizer = AutoModel.from_pretrained(model_name)    \n",
    "    model = TFBertForMultiClassClassification(model_name, class_num)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(\n",
    "    train.drop('class',axis=1), train['class'], test_size=0.2, random_state=42 , stratify=train['class']\n",
    ")\n",
    "# Aug\n",
    "# train_x, train_y = aug(train_x, train_y, 0)\n",
    "# train_x, train_y = aug(train_x, train_y, 1)\n",
    "# train_x, train_y = aug(train_x, train_y, 3)\n",
    "\n",
    "# 토크나이저\n",
    "train_X, train_Y = convert_examples_to_features(\n",
    "    train_x['conversation'], train_y, \n",
    "    max_seq_len=max_len, tokenizer=tokenizer\n",
    ")\n",
    "val_X, val_Y = convert_examples_to_features(\n",
    "    val_x['conversation'], val_y, \n",
    "    max_seq_len=max_len, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 옵티마이저, loss\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate= lr)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# model compile\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Define the learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "    if epoch == 0:\n",
    "        return lr\n",
    "    elif epoch == 1 :\n",
    "        return 0.00005 # 5e-5 , 0.00005\n",
    "    elif epoch == 2 :\n",
    "        return 0.000001\n",
    "    elif epoch == 3 :\n",
    "        return 0.000005\n",
    "    elif epoch == 4 :\n",
    "        return 0.0000001\n",
    "    elif epoch == 5 :\n",
    "        return 0.0000005\n",
    "    elif epoch == 6 :\n",
    "        return 0.00000001\n",
    "    else:\n",
    "        return 0.00000001\n",
    "\n",
    "# Create the LearningRateScheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    \n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_X, train_Y, \n",
    "    validation_data=(val_X,val_Y),\n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    #callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1fa530",
   "metadata": {},
   "source": [
    "- 수정사항 :\n",
    "    1. stratify=train['class'] 으로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633438a",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train_ai_last.csv'\n",
    "val_path = './aug_split_val.csv'\n",
    "model_name = 'klue/bert-base' # klue/roberta-large klue/bert-base\n",
    "class_num = 4 \n",
    "max_len = 200\n",
    "lr = 5e-5\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fad261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# 메모리 해제\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d849fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a19e56eb",
   "metadata": {},
   "source": [
    "# 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "676467b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Accuracy: 0.8974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.83      0.88       179\n",
      "     Class 1       0.82      0.96      0.89       195\n",
      "     Class 2       0.90      0.95      0.93       194\n",
      "     Class 3       0.95      0.84      0.89       202\n",
      "\n",
      "    accuracy                           0.90       770\n",
      "   macro avg       0.90      0.90      0.90       770\n",
      "weighted avg       0.90      0.90      0.90       770\n",
      "\n",
      "\n",
      "Weighted F1 Score (based on real predictions): 0.8971\n"
     ]
    }
   ],
   "source": [
    "# 데이터 기타 제외 가해자만 남기기 \n",
    "# 직장 제외 데이터 증강(랜덤 스위치), 데이터 삭제\n",
    "# 기타 에서 '돈' 키워드 제거 - 상승\n",
    "# 0번(협박)만 증강 2번\n",
    "# 제출 : 0.8275\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 실제 예측값 생성\n",
    "real_predictions = model.predict(val_X)\n",
    "\n",
    "# 예측값을 레이블로 변환\n",
    "real_predicted_labels = np.argmax(real_predictions, axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "real_accuracy = accuracy_score(val_Y, real_predicted_labels)\n",
    "print(f\"Real Accuracy: {real_accuracy:.4f}\")\n",
    "\n",
    "# 분류 보고서 생성\n",
    "real_report = classification_report(val_Y, real_predicted_labels, target_names=[f\"Class {i}\" for i in range(4)])\n",
    "print(real_report)\n",
    "\n",
    "# F1 스코어 계산\n",
    "real_f1 = f1_score(val_Y, real_predicted_labels, average='weighted')\n",
    "print(f\"\\nWeighted F1 Score (based on real predictions): {real_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcee581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "4ac4e1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Accuracy: 0.9130\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.87      0.87      0.87       179\n",
      "     Class 1       0.90      0.92      0.91       195\n",
      "     Class 2       0.97      0.94      0.95       194\n",
      "     Class 3       0.92      0.93      0.92       202\n",
      "\n",
      "    accuracy                           0.91       770\n",
      "   macro avg       0.91      0.91      0.91       770\n",
      "weighted avg       0.91      0.91      0.91       770\n",
      "\n",
      "\n",
      "Weighted F1 Score (based on real predictions): 0.9132\n"
     ]
    }
   ],
   "source": [
    "# 데이터 기타 제외 가해자만 남기기 \n",
    "# 직장 제외 데이터 증강(랜덤 스위치), 데이터 삭제\n",
    "# 기타 에서 '돈' 키워드 제거 - 상승\n",
    "# 제출 : 0.8575\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 실제 예측값 생성\n",
    "real_predictions = model.predict(val_X)\n",
    "\n",
    "# 예측값을 레이블로 변환\n",
    "real_predicted_labels = np.argmax(real_predictions, axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "real_accuracy = accuracy_score(val_Y, real_predicted_labels)\n",
    "print(f\"Real Accuracy: {real_accuracy:.4f}\")\n",
    "\n",
    "# 분류 보고서 생성\n",
    "real_report = classification_report(val_Y, real_predicted_labels, target_names=[f\"Class {i}\" for i in range(4)])\n",
    "print(real_report)\n",
    "\n",
    "# F1 스코어 계산\n",
    "real_f1 = f1_score(val_Y, real_predicted_labels, average='weighted')\n",
    "print(f\"\\nWeighted F1 Score (based on real predictions): {real_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a239946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "0e8cb615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Accuracy: 0.9078\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.88      0.85      0.87       179\n",
      "     Class 1       0.86      0.93      0.89       195\n",
      "     Class 2       0.96      0.94      0.95       194\n",
      "     Class 3       0.93      0.90      0.91       202\n",
      "\n",
      "    accuracy                           0.91       770\n",
      "   macro avg       0.91      0.91      0.91       770\n",
      "weighted avg       0.91      0.91      0.91       770\n",
      "\n",
      "\n",
      "Weighted F1 Score (based on real predictions): 0.9080\n"
     ]
    }
   ],
   "source": [
    "# 데이터 기타 제외 가해자만 남기기 \n",
    "# 직장 제외 데이터 증강(랜덤 스위치), 데이터 삭제\n",
    "# 제출 : 0.845\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 실제 예측값 생성\n",
    "real_predictions = model.predict(val_X)\n",
    "\n",
    "# 예측값을 레이블로 변환\n",
    "real_predicted_labels = np.argmax(real_predictions, axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "real_accuracy = accuracy_score(val_Y, real_predicted_labels)\n",
    "print(f\"Real Accuracy: {real_accuracy:.4f}\")\n",
    "\n",
    "# 분류 보고서 생성\n",
    "real_report = classification_report(val_Y, real_predicted_labels, target_names=[f\"Class {i}\" for i in range(4)])\n",
    "print(real_report)\n",
    "\n",
    "# F1 스코어 계산\n",
    "real_f1 = f1_score(val_Y, real_predicted_labels, average='weighted')\n",
    "print(f\"\\nWeighted F1 Score (based on real predictions): {real_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea5eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3706ebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Accuracy: 0.9026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.80      0.87       179\n",
      "     Class 1       0.87      0.87      0.87       195\n",
      "     Class 2       0.96      0.95      0.95       194\n",
      "     Class 3       0.85      0.98      0.91       202\n",
      "\n",
      "    accuracy                           0.90       770\n",
      "   macro avg       0.91      0.90      0.90       770\n",
      "weighted avg       0.91      0.90      0.90       770\n",
      "\n",
      "\n",
      "Weighted F1 Score (based on real predictions): 0.9019\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5376, 770]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_557/508211245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# 혼동 행렬 생성 및 표시\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mreal_conf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_predicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_conf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"Class {i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"Class {i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5376, 770]"
     ]
    }
   ],
   "source": [
    "# 데이터 기타 제외 가해자만 남기기 \n",
    "# 직장 제외 데이터 증강(랜덤 스위치)\n",
    "# 제출 : 0.76\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 실제 예측값 생성\n",
    "real_predictions = model.predict(val_X)\n",
    "\n",
    "# 예측값을 레이블로 변환\n",
    "real_predicted_labels = np.argmax(real_predictions, axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "real_accuracy = accuracy_score(val_Y, real_predicted_labels)\n",
    "print(f\"Real Accuracy: {real_accuracy:.4f}\")\n",
    "\n",
    "# 분류 보고서 생성\n",
    "real_report = classification_report(val_Y, real_predicted_labels, target_names=[f\"Class {i}\" for i in range(4)])\n",
    "print(real_report)\n",
    "\n",
    "# F1 스코어 계산\n",
    "real_f1 = f1_score(val_Y, real_predicted_labels, average='weighted')\n",
    "print(f\"\\nWeighted F1 Score (based on real predictions): {real_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7da8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483e961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "91ca5197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Accuracy: 0.8987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.89      0.88      0.88       179\n",
      "     Class 1       0.88      0.89      0.89       195\n",
      "     Class 2       1.00      0.87      0.93       194\n",
      "     Class 3       0.85      0.96      0.90       202\n",
      "\n",
      "    accuracy                           0.90       770\n",
      "   macro avg       0.90      0.90      0.90       770\n",
      "weighted avg       0.90      0.90      0.90       770\n",
      "\n",
      "\n",
      "Weighted F1 Score (based on real predictions): 0.8992\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3076, 770]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_557/1201338190.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# 혼동 행렬 생성 및 표시\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mreal_conf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_predicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_conf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"Class {i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"Class {i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3076, 770]"
     ]
    }
   ],
   "source": [
    "# 데이터 기타 제외 가해자만 남기기\n",
    "# 제출 : 0.805\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 실제 예측값 생성\n",
    "real_predictions = model.predict(val_X)\n",
    "\n",
    "# 예측값을 레이블로 변환\n",
    "real_predicted_labels = np.argmax(real_predictions, axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "real_accuracy = accuracy_score(val_Y, real_predicted_labels)\n",
    "print(f\"Real Accuracy: {real_accuracy:.4f}\")\n",
    "\n",
    "# 분류 보고서 생성\n",
    "real_report = classification_report(val_Y, real_predicted_labels, target_names=[f\"Class {i}\" for i in range(4)])\n",
    "print(real_report)\n",
    "\n",
    "# F1 스코어 계산\n",
    "real_f1 = f1_score(val_Y, real_predicted_labels, average='weighted')\n",
    "print(f\"\\nWeighted F1 Score (based on real predictions): {real_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Real Accuracy: 0.8987\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "     협박 0       0.89      0.88      0.88       179\n",
    "     갈취 1       0.88      0.89      0.89       195\n",
    "     직장 2       1.00      0.87      0.93       194\n",
    "     기타 3       0.85      0.96      0.90       202\n",
    "\n",
    "    accuracy                           0.90       770\n",
    "   macro avg       0.90      0.90      0.90       770\n",
    "weighted avg       0.90      0.90      0.90       770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1e5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcd712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a27f021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Accuracy: 0.8651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.92      0.82      0.87       178\n",
      "     Class 1       0.77      0.93      0.84       195\n",
      "     Class 2       0.91      0.95      0.93       194\n",
      "     Class 3       0.89      0.76      0.82       204\n",
      "\n",
      "    accuracy                           0.87       771\n",
      "   macro avg       0.87      0.87      0.87       771\n",
      "weighted avg       0.87      0.87      0.86       771\n",
      "\n",
      "\n",
      "Weighted F1 Score (based on real predictions): 0.8645\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3081, 771]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_557/1201338190.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# 혼동 행렬 생성 및 표시\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mreal_conf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_predicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_conf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"Class {i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"Class {i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3081, 771]"
     ]
    }
   ],
   "source": [
    "# 데이터 전체 가해자만 남기기\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 실제 예측값 생성\n",
    "real_predictions = model.predict(val_X)\n",
    "\n",
    "# 예측값을 레이블로 변환\n",
    "real_predicted_labels = np.argmax(real_predictions, axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "real_accuracy = accuracy_score(val_Y, real_predicted_labels)\n",
    "print(f\"Real Accuracy: {real_accuracy:.4f}\")\n",
    "\n",
    "# 분류 보고서 생성\n",
    "real_report = classification_report(val_Y, real_predicted_labels, target_names=[f\"Class {i}\" for i in range(4)])\n",
    "print(real_report)\n",
    "\n",
    "# F1 스코어 계산\n",
    "real_f1 = f1_score(val_Y, real_predicted_labels, average='weighted')\n",
    "print(f\"\\nWeighted F1 Score (based on real predictions): {real_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15c3ae",
   "metadata": {},
   "source": [
    "Real Accuracy: 0.8651\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "     협박(0)       0.92      0.82      0.87       178\n",
    "     갈취(1)       0.77      0.93      0.84       195\n",
    "     직장(2)       0.91      0.95      0.93       194\n",
    "     기타(3)       0.89      0.76      0.82       204\n",
    "\n",
    "    accuracy                           0.87       771\n",
    "   macro avg       0.87      0.87      0.87       771\n",
    "weighted avg       0.87      0.87      0.86       771\n",
    "\n",
    "\n",
    "- 협박(Class: 협박)\n",
    "\n",
    "\n",
    "상황: 협박 클래스의 경우 정밀도와 재현율이 비교적 높은 편입니다. 그러나 재현율이 약간 낮은 편이며, 이는 실제 협박 케이스 중에서 일부를 놓치고 있다는 의미입니다.\n",
    "해결 방법: 재현율을 높이기 위해 실제 협박 사례가 누락되지 않도록 `데이터를 추가`하는 것이 좋을 수 있습니다. 협박 클래스에 대한 추가 데이터를 확보하거나 수집하여 모델이 협박을 더 잘 인식하고 예측하도록 돕는 것이 가능합니다.\n",
    "\n",
    "- 갈취(Class: 갈취)\n",
    "\n",
    "상황: 갈취 클래스의 경우 재현율이 높은 편으로, 대부분의 갈취 사례를 모델이 예측하는 것으로 보입니다. 그러나 정밀도가 상대적으로 낮아, 모델이 갈취로 잘못 예측하는 경우가 있을 수 있습니다.\n",
    "해결 방법: 정밀도를 향상시키기 위해 모델이 갈취로 잘못 예측하는 경우를 줄일 필요가 있습니다. 이를 위해 추가 데이터 수집 대신 모델의 하이퍼파라미터를 조정하거나 `데이터 전처리`를 통해 모델이 갈취 클래스를 더 잘 구분하도록 돕는 것이 중요할 수 있습니다.\n",
    "- 직장(Class: 직장)\n",
    "\n",
    "상황: 직장 클래스의 경우 정밀도와 재현율이 높은 편으로, 모델이 직장 사례를 예측하는 데 잘 성공하고 있습니다.\n",
    "해결 방법: 현재 상태에서는 특별히 추가적인 데이터나 조치가 필요하지 않아 보입니다. 모델이 직장 클래스를 잘 예측하고 있으므로 유사한 성능을 유지하는 것이 중요합니다.\n",
    "\n",
    "- 기타(Class: 기타)\n",
    "\n",
    "상황: 기타 클래스의 경우 정밀도와 재현율이 상대적으로 높은 편입니다. 그러나 재현율이 낮은 편으로, 실제 기타 사례 중에서 일부를 놓치고 있다는 의미입니다.\n",
    "해결 방법: 재현율을 높이기 위해 기타 클래스의 실제 케이스가 누락되지 않도록 데이터를 추가하는 것이 도움이 될 수 있습니다. `기타 클래스에 대한 더 많은 다양한 예시를 모델이 학습하게 함으로써 모델의 일반화 능력을 향상`시킬 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7a8ea",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "834f8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features_test(examples, max_seq_len, tokenizer):\n",
    "    \n",
    "    input_ids, attention_masks, token_type_ids = [], [], []\n",
    "    \n",
    "    for example in tqdm(examples, total=len(examples)):\n",
    "        # input_id는 워드 임베딩을 위한 문장의 정수 인코딩\n",
    "        input_id = tokenizer.encode(example, max_length=max_seq_len, \n",
    "                                    pad_to_max_length=True)\n",
    "        \n",
    "        # attention_mask는 실제 단어가 위치하면 1, 패딩의 위치에는 0인 시퀀스\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "        \n",
    "        # token_type_id은 세그먼트 인코딩\n",
    "        token_type_id = [0] * max_seq_len\n",
    "        \n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "    \n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    \n",
    "    return (input_ids, attention_masks, token_type_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "77ac439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline(model, tokenizer, max_len):\n",
    "    \n",
    "    test_data_path = \"./data/test.json\"\n",
    "    with open(test_data_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        test_data = json.load(json_file)\n",
    "        \n",
    "    test = pd.DataFrame({'file_name':list(test_data.keys()), 'conversation': list(test_data.values())})\n",
    "    test['conversation'] = test['conversation'].apply(lambda x : x['text'])\n",
    "    \n",
    "    # preprocessing\n",
    "    test['conversation'] = test['conversation'].apply(preprocess_sentence)\n",
    "    \n",
    "    # 토크나이저\n",
    "    test_X = convert_examples_to_features_test(\n",
    "        test['conversation'],\n",
    "        max_seq_len=max_len, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # 예측\n",
    "    predictions = model.predict(test_X)\n",
    "    test_class_probabilities = tf.nn.softmax(predictions, axis=-1).numpy() # [[0.13297564 0.8358507  0.00801584 0.02315779]]\n",
    "    test_predicted_class = np.argmax(test_class_probabilities, axis=1) # [ 1 ]\n",
    "    \n",
    "    return test_predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "4718407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 400/400 [00:00<00:00, 1110.73it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = test_pipeline(model, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "074b80f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 3, 1, 2, 1, 3, 0, 2, 2, 3, 0, 3, 0, 3, 3, 1, 1, 2, 2, 1,\n",
       "       1, 1, 3, 3, 3, 1, 0, 1, 0, 1, 2, 1, 2, 0, 3, 0, 1, 3, 1, 2, 2, 3,\n",
       "       3, 3, 3, 1, 3, 2, 3, 2, 0, 3, 3, 1, 2, 3, 2, 0, 2, 1, 2, 0, 3, 3,\n",
       "       2, 1, 1, 2, 2, 0, 3, 2, 2, 0, 3, 1, 0, 3, 2, 2, 3, 0, 0, 0, 2, 1,\n",
       "       1, 2, 2, 1, 2, 3, 1, 1, 1, 1, 2, 1, 3, 3, 2, 3, 1, 0, 3, 0, 1, 0,\n",
       "       1, 3, 2, 0, 0, 0, 3, 1, 0, 0, 0, 2, 3, 0, 3, 0, 2, 2, 3, 0, 2, 2,\n",
       "       3, 3, 3, 0, 2, 3, 1, 2, 0, 2, 2, 0, 0, 1, 3, 0, 1, 2, 1, 3, 0, 3,\n",
       "       3, 2, 0, 3, 3, 0, 3, 2, 1, 3, 2, 1, 1, 2, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 2, 0, 3, 3, 1, 3, 3, 1, 0, 3, 3, 3, 3, 2, 1, 2, 1, 2, 1, 0,\n",
       "       0, 2, 2, 3, 2, 1, 2, 1, 1, 3, 3, 1, 1, 0, 2, 2, 2, 3, 2, 2, 1, 0,\n",
       "       3, 2, 2, 1, 2, 1, 3, 1, 1, 2, 3, 2, 3, 1, 0, 3, 0, 2, 1, 1, 0, 2,\n",
       "       1, 2, 1, 1, 3, 2, 3, 1, 3, 3, 1, 2, 0, 3, 1, 1, 1, 3, 2, 3, 3, 0,\n",
       "       0, 3, 1, 3, 1, 3, 0, 2, 0, 1, 1, 2, 1, 0, 3, 0, 2, 1, 1, 3, 3, 3,\n",
       "       1, 3, 3, 1, 1, 2, 1, 2, 3, 3, 1, 3, 0, 2, 2, 1, 1, 0, 1, 1, 2, 0,\n",
       "       2, 2, 3, 1, 0, 0, 3, 1, 0, 1, 2, 1, 3, 0, 1, 3, 3, 3, 2, 2, 3, 3,\n",
       "       2, 1, 3, 3, 0, 2, 3, 1, 0, 1, 1, 0, 3, 3, 1, 1, 2, 0, 3, 1, 0, 0,\n",
       "       2, 3, 0, 2, 1, 2, 0, 2, 2, 1, 0, 2, 3, 3, 2, 3, 3, 1, 1, 1, 2, 2,\n",
       "       1, 3, 2, 0, 2, 3, 0, 3, 2, 1, 2, 3, 1, 0, 1, 3, 0, 3, 0, 1, 1, 2,\n",
       "       2, 1, 0, 0])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "545d221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/submission.csv')\n",
    "submission['class'] = predictions\n",
    "submission.to_csv('submissions/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021562ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
